{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring of the Autobiographical Interview with NLP\n",
    "\n",
    "This code is used to automatically score text data using natural language processing. The trained language model of Genugten and Schacter (2024) is employed to score English text data in accordance with the Autobiographical Interview method (Levine, 2002). \n",
    "\n",
    "## License and Copyright Note\n",
    "\n",
    "The code is based on the [Colab Notebook](https://colab.research.google.com/github/rubenvangenugten/autobiographical_interview_scoring/blob/main/automated_autobiographical_interview_scoring_share.ipynb) by Ruben von Genugten, published on [GitHub](https://github.com/rubenvangenugten/autobiographical_interview_scoring) under the GPL-3.0 licence. It has been modified in order to adapt it to our study and data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install packages if necessary\n",
    "!pip install numpy\n",
    "!pip install tensorflow\n",
    "!pip install pysbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pysbd\n",
    "import re\n",
    "from transformers import AutoTokenizer, TFDistilBertForSequenceClassification\n",
    "from transformers import TextClassificationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "marian = pd.read_csv(\"../data/translated/marian.csv\")\n",
    "mbart = pd.read_csv(\"../data/translated/mbart.csv\")\n",
    "google = pd.read_csv(\"../data/translated/google.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access trained model and tokenizer\n",
    "# this model was trained and used by Genugten & Schacter 2024 \n",
    "aiscoring = 'vangenugtenr/autobiographical_interview_scoring'\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(aiscoring)\n",
    "tokenizer = AutoTokenizer.from_pretrained(aiscoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we create a data frame for each data set with the text data in long format \n",
    "# the long format contains one row per sentence that can then be classified \n",
    "\n",
    "# define sentence segmenter\n",
    "seg = pysbd.Segmenter(language=\"en\", clean=False)\n",
    "\n",
    "# define function \n",
    "def reshape_to_long_format(data):\n",
    "    list_of_dataframes = []\n",
    "    for row in range(data.shape[0]):\n",
    "        # access some general info about this narrative\n",
    "        this_subID = data.iloc[row, data.columns.get_loc(\"ID\")]\n",
    "        narrative = data.iloc[row, data.columns.get_loc(\"text_eng\")]\n",
    "        # store current row\n",
    "        currentRow = data.iloc[[row], :]\n",
    "        # create a new dataframe with each row a new sentence, and subID added\n",
    "        segmented_sentences = seg.segment(narrative)\n",
    "        sentences_df = pd.DataFrame(segmented_sentences, columns=['sentence'])\n",
    "        sentences_df[\"ID\"] = this_subID\n",
    "        # create a new merged dataframe\n",
    "        merged_thisNarrative = pd.merge(currentRow, sentences_df, on=[\"ID\"])\n",
    "        list_of_dataframes.append(merged_thisNarrative)\n",
    "    return pd.concat(list_of_dataframes)\n",
    "\n",
    "# here we process each of our data sets separately using the defined function\n",
    "marian_long = reshape_to_long_format(marian)\n",
    "mbart_long = reshape_to_long_format(mbart)\n",
    "google_long = reshape_to_long_format(google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we prepare the data sets for the model \n",
    "# that means, the data are shaped such, that BERT is able to work with them \n",
    "\n",
    "# define data type, which should be character \n",
    "marian_long.loc[:,'sentence'] = marian_long.loc[:,'sentence'].astype('str')\n",
    "mbart_long.loc[:,'sentence'] = mbart_long.loc[:,'sentence'].astype('str')\n",
    "google_long.loc[:,'sentence'] = google_long.loc[:,'sentence'].astype('str')\n",
    "\n",
    "# define a function \n",
    "def prepare(data):\n",
    "    test_texts = []\n",
    "    # extract each sentence, convert to string, and append to list\n",
    "    for row in range(data.shape[0]):\n",
    "        temp_text = data.iloc[row, data.columns.get_loc(\"sentence\")]\n",
    "        temp_text = str(temp_text)  \n",
    "        test_texts.append(temp_text)\n",
    "    # encode text for BERT model\n",
    "    encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "    # convert to a TensorFlow dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(encodings)))\n",
    "    return dataset, test_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we classify the sentences \n",
    "\n",
    "# set up text classification pipeline using the defined model and tokenizer\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)\n",
    "\n",
    "def classification(data):\n",
    "    # use the preparation function from above \n",
    "    dataset, test_texts = prepare(data)\n",
    "    # Split classification up into batches of sentences to manage RAM\n",
    "    stored_test = []\n",
    "    batch_size = 200\n",
    "    # unse the classification pipeline \n",
    "    for i in range(0, len(test_texts), batch_size):\n",
    "      stored_test.extend(pipe(test_texts[i:i+batch_size]))\n",
    "    return stored_test\n",
    "\n",
    "# here we process our data sets using the function\n",
    "marian_classified = classification(marian_long)\n",
    "mbart_classified = classification(mbart_long)\n",
    "google_classified = classification(google_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we generate new dataframes with predictions \n",
    "\n",
    "def predictions(data, stored_test):\n",
    "    # create a list to store prediction dataframes\n",
    "    list_of_predictionDfs = []\n",
    "    # For each item in the stored_test (predictions), create a data frame and process\n",
    "    for row in range(len(stored_test)):\n",
    "        thisTestLabels = pd.DataFrame(stored_test[row])\n",
    "        # set the 'label' as the index and remove it from the columns\n",
    "        thisTestLabels.index = thisTestLabels['label']\n",
    "        thisTestLabels = thisTestLabels.drop('label', axis=1)\n",
    "        thisTestLabels = thisTestLabels.transpose()\n",
    "        # append the data frame to the list\n",
    "        list_of_predictionDfs.append(thisTestLabels)\n",
    "    # get the prediction data frames \n",
    "    predictionsDf = pd.concat(list_of_predictionDfs)\n",
    "    # identify the most likely label for each sentence\n",
    "    predictionsDf['toplabel'] = predictionsDf.idxmax(axis=1)\n",
    "    # merge the predictions with the original data frame \n",
    "    merged_data = pd.concat([data.reset_index(drop=True), predictionsDf.reset_index(drop=True)], axis=1)\n",
    "    # add a variable with a word count for each sentence\n",
    "    merged_data['sentenceWordCount'] = merged_data['sentence'].apply(lambda x: len(re.findall(r'\\w+', str(x))))\n",
    "    return merged_data\n",
    "\n",
    "marian_predictions = predictions(marian_long, marian_classified)\n",
    "mbart_predictions = predictions(mbart_long, mbart_classified)\n",
    "google_predictions = predictions(google_long, google_classified)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marian_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we create to variables wit predicted classifications\n",
    "# number of words classified as internal, and number of words classified as external \n",
    "\n",
    "def predicted_words(df):\n",
    "    # create two new columns for the counts\n",
    "    df[['internal_pred']] = 0\n",
    "    df[['external_pred']] = 0\n",
    "    # loop through each row and calculate the counts\n",
    "    for row in range(df.shape[0]):\n",
    "        predictionType_thisIter = df.iloc[row, df.columns.get_loc(\"toplabel\")]\n",
    "        numTotalWords = df.iloc[row, df.columns.get_loc(\"sentenceWordCount\")]\n",
    "        # get the column locations for internal and external predictions\n",
    "        internalLocation = df.columns.get_loc(\"internal_pred\")\n",
    "        externalLocation = df.columns.get_loc(\"external_pred\")\n",
    "        # classify based on the label and update the columns\n",
    "        if predictionType_thisIter == 'LABEL_0':\n",
    "            df.iloc[row, externalLocation] = numTotalWords\n",
    "        elif predictionType_thisIter == 'LABEL_1':\n",
    "            halfDetails = numTotalWords / 2\n",
    "            df.iloc[row, externalLocation] = halfDetails\n",
    "            df.iloc[row, internalLocation] = halfDetails\n",
    "        elif predictionType_thisIter == 'LABEL_2':\n",
    "            df.iloc[row, externalLocation] = numTotalWords / 4\n",
    "            df.iloc[row, internalLocation] = numTotalWords * (3 / 4)\n",
    "        elif predictionType_thisIter == 'LABEL_3':\n",
    "            df.iloc[row, internalLocation] = numTotalWords\n",
    "    return df\n",
    "\n",
    "# here we apply the function to our data frames \n",
    "marian_new = predicted_words(marian_predictions)\n",
    "mbart_new = predicted_words(mbart_predictions)\n",
    "google_new = predicted_words(google_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marian_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum up internal and external words for each narrative\n",
    "# because right now we are still on sentence level\n",
    "\n",
    "def sum_narrative(df):\n",
    "    # select the relevant columns for output\n",
    "    df_write_out_subset = df.loc[:, [\"ID\", \"internal_pred\", \"external_pred\", 'sentenceWordCount']]\n",
    "    # Group by ID, then sum the 'internal_pred' and 'external_pred' columns\n",
    "    grouped = df_write_out_subset.groupby(by=[\"ID\"]).agg({\n",
    "        'internal_pred': 'sum',\n",
    "        'external_pred': 'sum',\n",
    "        'sentenceWordCount': 'sum'\n",
    "    }).reset_index()  # reset index to get a clean dataframe\n",
    "    grouped.rename(columns={\"sentenceWordCount\": \"total_words\"}, inplace=True)\n",
    "    return grouped\n",
    "\n",
    "# here we apply the function to our data frames \n",
    "marian_grouped = sum_narrative(marian_new)\n",
    "mbart_grouped = sum_narrative(mbart_new)\n",
    "google_grouped = sum_narrative(google_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data sets into csv files\n",
    "marian_grouped.to_csv('../data/scored/marian_scored.csv', index=False)\n",
    "mbart_grouped.to_csv('../data/scored/mbart_scored.csv', index=False)\n",
    "google_grouped.to_csv('../data/scored/google_scored.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
