{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78de972a",
   "metadata": {},
   "source": [
    "The following script allows for quick and flexible translation and subsequent scoring of non-English texts with the Autobiographical Interview (Levene, 2005) heavily relying on the automated-scoring approach developed and provided by van Genugten & Schacter (2024).  \n",
    "It is a generalization and extension of the code used by Annika Kuelpmann for this project: https://osf.io/vcns4.  \n",
    "The liscence for this script is:   \n",
    "The authors of this script are: Giuliano Groer (https://orcid.org/0009-0009-2656-3796), Annika Kuelpmann (https://orcid.org/0000-0002-0256-2037), Laurin Plank (https://orcid.org/0000-0002-8846-5405)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f1fb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e33ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#installing the necessary packages via requirements.txt\n",
    "# Path to requirements.txt\n",
    "requirements_file = \"requirements.txt\"\n",
    "\n",
    "if os.path.exists(requirements_file):\n",
    "    print(\"Found requirements.txt, installing packages...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", requirements_file])\n",
    "else:\n",
    "    print(\"No requirements.txt found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0909052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Giuliano\n",
      "[nltk_data]     Groer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#loading packages\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import pysbd\n",
    "\n",
    "from textblob_de import TextBlobDE\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    TFDistilBertForSequenceClassification,\n",
    "    TextClassificationPipeline,\n",
    "    MarianMTModel,\n",
    "    MarianTokenizer,\n",
    "    MBartForConditionalGeneration,\n",
    "    MBart50Tokenizer\n",
    ")\n",
    "\n",
    "\n",
    "#vangenugten model\n",
    "aiscoring = 'vangenugtenr/autobiographical_interview_scoring'\n",
    "model_genugten = TFDistilBertForSequenceClassification.from_pretrained(aiscoring)\n",
    "tokenizer = AutoTokenizer.from_pretrained(aiscoring)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbde6323",
   "metadata": {},
   "source": [
    "load in your data.\n",
    "your data should contain a column containing the text to be translated called \"response\", a column containing the ID of the subject that provided the response called \"ResponseId\", and a column containing the number of the response (1 if it is only one per subject) called \"response_number\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d12a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_data = pd.read_csv(\"LOCATION_OF_YOUR_DATAFILE.csv\", sep= \",\",header = 0, decimal='.', encoding= 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bdb831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Unified translator function ---\n",
    "def translate_row(row, translator_name):\n",
    "    \"\"\"\n",
    "    row: a DataFrame row with 'response', 'ResponseId', 'response_number'\n",
    "    translator_name: 'mbart', 'marian', or 'google'\n",
    "    \"\"\"\n",
    "    text = row['response']\n",
    "    \n",
    "    if translator_name == \"mbart\":\n",
    "        mbart_model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "        model_mbart = MBartForConditionalGeneration.from_pretrained(mbart_model_name)\n",
    "        tokenizer_mbart = MBart50Tokenizer.from_pretrained(mbart_model_name)\n",
    "        tokenizer_mbart.src_lang = \"de_DE\"\n",
    "        inputs = tokenizer_mbart(text, return_tensors=\"pt\", truncation=True)\n",
    "        translated_tokens = model_mbart.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tokenizer_mbart.lang_code_to_id[\"en_XX\"]\n",
    "        )\n",
    "        return tokenizer_mbart.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "    elif translator_name == \"marian\":\n",
    "        marian_model_name = \"Helsinki-NLP/opus-mt-de-en\"\n",
    "        model_marian = MarianMTModel.from_pretrained(marian_model_name)\n",
    "        tokenizer_marian = MarianTokenizer.from_pretrained(marian_model_name)\n",
    "        inputs = tokenizer_marian(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        translated_tokens = model_marian.generate(**inputs)\n",
    "        return tokenizer_marian.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "    elif translator_name == \"google\":\n",
    "        google_translator = GoogleTranslator(source='de', target='en')\n",
    "        return google_translator.translate(text)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Translator must be 'mbart', 'marian', or 'google'\")\n",
    "\n",
    "\n",
    "def translate_dataframe(df, translator_name):\n",
    "    df = df.copy()\n",
    "    translations = []\n",
    "\n",
    "    # Translate each row with a progress bar\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=f"Translating with {translator_name}"):\n",
    "        translations.append(translate_row(row, translator_name))\n",
    "\n",
    "    df['translation'] = translations\n",
    "\n",
    "    # Add the new ID_new column\n",
    "    df[\"ID_new\"] = df[\"ResponseId\"].astype(str) + \"_\" + df[\"response_number\"].astype(str)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7ed00e",
   "metadata": {},
   "source": [
    "Simply put your data in and specify a translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635c1cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator_choice = \"mbart\"  # or \"marian\", or \"google\"\n",
    "\n",
    "# Translate entire DataFrame\n",
    "translated_language_data = translate_dataframe(language_data, \"mbart\")  # or \"marian\" / \"google\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd63702",
   "metadata": {},
   "source": [
    "For Scoring, use the below code segments.\n",
    "The code is a modified version of the code provided by van Genugten & Schacter (2024) using their model and general coding approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca3cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- helper functions to allow scoring --- \n",
    "seg = pysbd.Segmenter(language=\"en\", clean=False)\n",
    "pipe = TextClassificationPipeline(model=model_genugten, tokenizer=tokenizer, return_all_scores=True)\n",
    "\n",
    "def reshape_to_long_format(data):\n",
    "    list_of_dataframes = []\n",
    "    for row in range(data.shape[0]):\n",
    "        # access some general info about this narrative\n",
    "        this_subID = data.iloc[row, data.columns.get_loc(\"ID_new\")]\n",
    "        narrative = data.iloc[row, data.columns.get_loc(\"response_eng\")]\n",
    "        # store current row\n",
    "        currentRow = data.iloc[[row], :]\n",
    "        # create a new dataframe with each row a new sentence, and subID added\n",
    "        segmented_sentences = seg.segment(narrative)\n",
    "        sentences_df = pd.DataFrame(segmented_sentences, columns=['sentence'])\n",
    "        sentences_df[\"ID_new\"] = this_subID\n",
    "        # create a new merged dataframe\n",
    "        merged_thisNarrative = pd.merge(currentRow, sentences_df, on=[\"ID_new\"])\n",
    "        list_of_dataframes.append(merged_thisNarrative)\n",
    "    return pd.concat(list_of_dataframes)\n",
    "\n",
    "def prepare(data):\n",
    "    test_texts = []\n",
    "    # extract each sentence, convert to string, and append to list\n",
    "    for row in range(data.shape[0]):\n",
    "        temp_text = data.iloc[row, data.columns.get_loc(\"sentence\")]\n",
    "        temp_text = str(temp_text)  \n",
    "        test_texts.append(temp_text)\n",
    "    # encode text for BERT model\n",
    "    encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "    # convert to a TensorFlow dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(encodings)))\n",
    "    return dataset, test_texts\n",
    "\n",
    "def classification(data):\n",
    "    # use the preparation function from above \n",
    "    dataset, test_texts = prepare(data)\n",
    "    # Split classification up into batches of sentences to manage RAM\n",
    "    stored_test = []\n",
    "    batch_size = 200\n",
    "    # unse the classification pipeline \n",
    "    for i in range(0, len(test_texts), batch_size):\n",
    "      stored_test.extend(pipe(test_texts[i:i+batch_size]))\n",
    "    return stored_test\n",
    "\n",
    "def predictions(data, stored_test):\n",
    "    # create a list to store prediction dataframes\n",
    "    list_of_predictionDfs = []\n",
    "    # For each item in the stored_test (predictions), create a data frame and process\n",
    "    for row in range(len(stored_test)):\n",
    "        thisTestLabels = pd.DataFrame(stored_test[row])\n",
    "        # set the 'label' as the index and remove it from the columns\n",
    "        thisTestLabels.index = thisTestLabels['label']\n",
    "        thisTestLabels = thisTestLabels.drop('label', axis=1)\n",
    "        thisTestLabels = thisTestLabels.transpose()\n",
    "        # append the data frame to the list\n",
    "        list_of_predictionDfs.append(thisTestLabels)\n",
    "    # get the prediction data frames \n",
    "    predictionsDf = pd.concat(list_of_predictionDfs)\n",
    "    # identify the most likely label for each sentence\n",
    "    predictionsDf['toplabel'] = predictionsDf.idxmax(axis=1)\n",
    "    # merge the predictions with the original data frame \n",
    "    merged_data = pd.concat([data.reset_index(drop=True), predictionsDf.reset_index(drop=True)], axis=1)\n",
    "    # add a variable with a word count for each sentence\n",
    "    merged_data['sentenceWordCount'] = merged_data['sentence'].apply(lambda x: len(re.findall(r'\\w+', str(x))))\n",
    "    return merged_data\n",
    "\n",
    "def predicted_words(df):\n",
    "    # create two new columns for the counts\n",
    "    df[['internal_pred']] = 0\n",
    "    df[['external_pred']] = 0\n",
    "    # loop through each row and calculate the counts\n",
    "    for row in range(df.shape[0]):\n",
    "        predictionType_thisIter = df.iloc[row, df.columns.get_loc(\"toplabel\")]\n",
    "        numTotalWords = df.iloc[row, df.columns.get_loc(\"sentenceWordCount\")]\n",
    "        # get the column locations for internal and external predictions\n",
    "        internalLocation = df.columns.get_loc(\"internal_pred\")\n",
    "        externalLocation = df.columns.get_loc(\"external_pred\")\n",
    "        # classify based on the label and update the columns\n",
    "        if predictionType_thisIter == 'LABEL_0':\n",
    "            df.iloc[row, externalLocation] = numTotalWords\n",
    "        elif predictionType_thisIter == 'LABEL_1':\n",
    "            halfDetails = numTotalWords / 2\n",
    "            df.iloc[row, externalLocation] = halfDetails\n",
    "            df.iloc[row, internalLocation] = halfDetails\n",
    "        elif predictionType_thisIter == 'LABEL_2':\n",
    "            df.iloc[row, externalLocation] = numTotalWords / 4\n",
    "            df.iloc[row, internalLocation] = numTotalWords * (3 / 4)\n",
    "        elif predictionType_thisIter == 'LABEL_3':\n",
    "            df.iloc[row, internalLocation] = numTotalWords\n",
    "    return df\n",
    "\n",
    "def sum_narrative(df):\n",
    "    # select the relevant columns for output\n",
    "    df_write_out_subset = df.loc[:, [\"ID_new\", \"internal_pred\", \"external_pred\", 'sentenceWordCount']]\n",
    "    # Group by ID, then sum the 'internal_pred' and 'external_pred' columns\n",
    "    grouped = df_write_out_subset.groupby(by=[\"ID_new\"]).agg({\n",
    "        'internal_pred': 'sum',\n",
    "        'external_pred': 'sum',\n",
    "        'sentenceWordCount': 'sum'\n",
    "    }).reset_index()  # reset index to get a clean dataframe\n",
    "    grouped.rename(columns={\"sentenceWordCount\": \"total_words\"}, inplace=True)\n",
    "    return grouped\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4239d6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- core function to score the language data ---\n",
    "def process_translated_data(translated_language_data, pipe):\n",
    "    \"\"\"\n",
    "    Full pipeline to process translated language data:\n",
    "    - reshape narratives to sentence-level\n",
    "    - classify sentences\n",
    "    - map predicted labels to word counts\n",
    "    - aggregate by narrative\n",
    "    \"\"\"\n",
    "    # Reshape to long format\n",
    "    translated_language_data_long = reshape_to_long_format(translated_language_data)\n",
    "\n",
    "    # Classify sentences using BERT pipeline\n",
    "    stored_test = classification(translated_language_data_long, pipe)\n",
    "\n",
    "    # Merge predictions with original long data\n",
    "    translated_language_data_predictions = predictions(translated_language_data_long, stored_test)\n",
    "\n",
    "    # Map labels to predicted internal/external word counts\n",
    "    translated_language_data_new = predicted_words(translated_language_data_predictions)\n",
    "\n",
    "    # Aggregate predictions by narrative\n",
    "    translated_language_data_grouped = sum_narrative(translated_language_data_new)\n",
    "\n",
    "    return translated_language_data_grouped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63a869d",
   "metadata": {},
   "source": [
    "Simply put your translated data into the following function to get the scored data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ebce65",
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_data = process_translated_data(translated_language_data, pipe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emb_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
